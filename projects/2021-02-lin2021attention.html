<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms | STARS Lab</title><meta name="generator" content="Jekyll v3.8.6" /><meta property="og:title" content="Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms" /><meta name="author" content="Lei Lin, Weizi Li, Huikun Bi, and Lingqiao Qin" /><meta property="og:locale" content="en_US" /><meta name="description" content="Lei Lin1, Weizi Li2, Huikun Bi3, and Lingqiao Qin4 1University of Rochester 2University of Memphis 3Institute of Computing Technology, Chinese Academy of Sciences 4University of Wisconsin-Madison" /><meta property="og:description" content="Lei Lin1, Weizi Li2, Huikun Bi3, and Lingqiao Qin4 1University of Rochester 2University of Memphis 3Institute of Computing Technology, Chinese Academy of Sciences 4University of Wisconsin-Madison" /><link rel="canonical" href="http://localhost:4000/projects/2021-02-lin2021attention" /><meta property="og:url" content="http://localhost:4000/projects/2021-02-lin2021attention" /><meta property="og:site_name" content="STARS Lab" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-05T11:54:45-05:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms" /><meta name="twitter:site" content="@ronaldsvilcins" /><meta name="twitter:creator" content="@Lei Lin, Weizi Li, Huikun Bi, and Lingqiao Qin" /> <script type="application/ld+json"> {"@type":"BlogPosting","url":"http://localhost:4000/projects/2021-02-lin2021attention","headline":"Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms","dateModified":"2022-08-05T11:54:45-05:00","datePublished":"2022-08-05T11:54:45-05:00","author":{"@type":"Person","name":"Lei Lin, Weizi Li, Huikun Bi, and Lingqiao Qin"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/2021-02-lin2021attention"},"description":"Lei Lin1, Weizi Li2, Huikun Bi3, and Lingqiao Qin4 1University of Rochester 2University of Memphis 3Institute of Computing Technology, Chinese Academy of Sciences 4University of Wisconsin-Madison","@context":"https://schema.org"}</script><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="STARS Lab" href="/atom.xml"><link rel="alternate" title="STARS Lab" type="application/json" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:500" rel="stylesheet"><style type="text/css"> *,*:before,*:after{box-sizing:inherit;margin:0;padding:0}html{box-sizing:border-box}body{font-family:'Cormorant Garamond', serif;background-color:#fafafa;color:#000;font-size:1rem;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility}a,a:visited{border:0;padding:0;text-decoration:none;color:#003087;font-weight:600}a:hover{color:#666}h1,h2{text-align:center;font-weight:500}h3{font-weight:500}ul{list-style-type:square;margin-left:2rem}header,main{margin:0 auto;max-width:50rem}img{width:100%}footer{font-family:'Inter', 'Helvetica', sans-serif;margin:1rem 0;text-align:center}footer a,footer a:visited{color:black;font-weight:normal}.section{position:relative}.section a{opacity:1;-webkit-transition:opacity .2s}.section a:hover{opacity:0.8;-webkit-transition:opacity .2s}.imglabel{position:absolute;top:50%;left:50%;transform:translate(-50%, -50%);visibility:hidden;opacity:0;transition:opacity .2s}.section:hover .imglabel{visibility:visible;opacity:1;transition:opacity .2s}.label{font-family:'Inter', 'Helvetica', sans-serif;font-size:1.8rem;color:white;font-weight:400;transition:.2s}.logo{height:7rem;width:auto;margin-top:2rem;margin-bottom:.5rem}.webicon{width:1rem}.trigram{width:1.1rem;margin-right:.5rem}.alignright{text-align:right;margin-right:1rem}.alignleft{text-align:left}.aligncenter{text-align:center}.caption{font-size:1.1rem;font-style:italic;text-align:center}.thumbnail{width:168px;height:120px}.thumbnailright{width:168px;height:120px;margin-left:auto}.itemgrid{display:flex;flex-wrap:wrap;display:grid;grid-template-columns:1fr;grid-auto-rows:minmax(0rem, auto);grid-gap:1.2rem}.item{display:flex;align-items:left;justify-content:left;position:relative;flex:1 1 10rem}.frame{padding:1rem;font-size:1.3rem;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);background-color:white;border-radius:10px}.frame h2{font-size:2rem}.frame h3{font-weight:0}.members{justify-content:center;text-align:center;display:flex;flex-wrap:wrap}.person{width:14.6rem;margin:0 0 1.2rem 0}.headshot{width:150px}.role{font-size:1.2rem;color:#666}.news{margin:0 1.4rem 0}.news p{margin:0 0 0.6rem 0}.news a{color:#41b6e6}.newstitle{font-size:1.5rem;color:#003087;font-weight:600}.newsinfo{font-size:0.8rem;text-transform:uppercase;letter-spacing:.1em;font-family:'Inter', 'Helvetica', sans-serif;color:#999}.newscontent{font-size:.9rem;line-height:1.3rem;font-family:'Inter', 'Helvetica', sans-serif;color:#666}.paper{margin:0 1.4rem 0}.paper p{margin:0 0 .2rem 0}.papertitle{font-size:1.3rem}.paperauthor{font-size:1.2rem}.papervenue{font-style:italic;font-size:1.1rem}#slider{overflow:hidden}#slider figure{position:relative;width:500%;margin:0;left:0;animation:20s sliding infinite}#slider figure img{width:20%;float:left}@keyframes sliding{0%{left:0}20%{left:0}25%{left:-100%}45%{left:-100%}50%{left:-200%}70%{left:-200%}75%{left:-300%}95%{left:-300%}100%{left:-400%}}blockquote{background:#f9f9f9;border-left:5px solid black;font-size:120%;margin:2rem 0;padding:1rem}blockquote p{margin:0}blockquote footer{font-size:80%;margin:1rem 0 0 0}dl dt{margin-bottom:0.5rem}dl dd{font-style:italic;margin-bottom:2rem}code,.highlight{background:#edf2f7;padding:0.1rem 0.3rem;font-size:1.1rem}pre{background:#edf2f7;padding:1em;font-size:.8rem;overflow:scroll}.iframe{position:relative;width:100%;overflow:hidden;padding-top:56.25%}.responsive{position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%;border:none}</style></head><body><header></header><main id="main" role="main"> <a href="/"><img class="logo" src="/images/logo/p.png"></a><div class="frame"><h2>Vehicle Trajectory Prediction Using LSTMs with Spatial-Temporal Attention Mechanisms</h2><br><p class="aligncenter">IEEE Intelligent Transportation Systems Magazine, 2021</p><br><p class="aligncenter"> <a href="http://urdata.net/">Lei Lin</a><sup>1</sup>, <a href="https://weizi-li.github.io/">Weizi Li</a><sup>2</sup>, <a href="https://huikunbi.github.io/index.html">Huikun Bi</a><sup>3</sup>, and <a href="https://www.linkedin.com/in/lingqiao-qin/">Lingqiao Qin</a><sup>4</sup> <br /> <sup>1</sup>University of Rochester <br /> <sup>2</sup>University of Memphis <br /> <sup>3</sup>Institute of Computing Technology, Chinese Academy of Sciences <br /> <sup>4</sup>University of Wisconsin-Madison</p><p><br /></p><p><img src="/projects/Lin2021Attention/teaser.png" alt="teaser" /></p><p class="aligncenter"> Model architecture</p><p><br /></p><p><img src="/projects/Lin2021Attention/teaser2.png" alt="teaser" /></p><p class="aligncenter"> Spatial-level attention weights by vehicle density</p><p><br /></p><h3 id="abstract">Abstract</h3><p>Accurate vehicle trajectory prediction can benefit a variety of Intelligent Transportation System applications ranging from traffic simulation to driver assistance. The need of this ability is pronounced with the emergence of autonomous vehicles, as they require the prediction of nearby vehiclesâ€™ trajectories in order to navigate safely and efficiently. Recent studies based on deep learning have greatly improved the prediction accuracy. However, one prominent issue of these models is the lack of model explainability. We alleviate this issue by proposing STA-LSTM, an LSTM model with spatial-temporal attention mechanisms for explainability in vehicle trajectory prediction. STA-LSTM not only achieves comparable prediction performance against other state-of-the-art models, but more importantly, explains the influence of historical trajectories and neighboring vehicles on the target vehicle. We provide in-depth analyses of the learned spatial-temporal attention weights in various highway scenarios based on different vehicle and environment factors, including target vehicle class, target vehicle location, and traffic density. A demonstration showing that STA-LSTM can capture and explain fine-grained lane-changing behaviors is also provided.</p><p><br /></p><h3 id="links">Links</h3><ul><li><a href="/projects/Lin2021Attention/Lin2021Attention.pdf">Preprint</a></li><li><a href="https://ieeexplore.ieee.org/document/9349962">Publication</a></li><li><a href="https://github.com/leilin-research/VTP">Code</a></li></ul><p><br /></p><h3 id="citation">Citation</h3><pre>
@Article{Lin2021Attention,
  author = {Lei Lin and Weizi Li and Huikun Bi and Lingqiao Qin},
  title = {Vehicle Trajectory Prediction Using {LSTM}s with Spatial-Temporal Attention Mechanisms},
  journal = {IEEE Intelligent Transportation Systems Magazine},
  year = {2021},
}
</pre><p><br /></p><h3 id="contact">Contact</h3><p>Lei Lin (lei.Lin@ieee.org) and Weizi Li (wli@memphis.edu)</p><p><br /></p><h3 id="acknowledgements">Acknowledgements</h3><p>The authors would like to thank the University of Memphis for providing the start-up fund.</p><br> <br><div> <a href="/projects">&#8592; All Publications</a></div></div><br><hr style="height:2px; border:none; background-color:#e7e9ee;"></main><footer class="footer" role="contentinfo"> <small>&copy; <a href="/">STARS Lab</a></small></footer></body></html>
